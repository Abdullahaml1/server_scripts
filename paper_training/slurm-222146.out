Start Training ...................................

Initializing TSN with base model: resnet18.
TSN Configurations:
    input_modality:     RGB
    num_segments:       3
    new_length:         1
    consensus_module:   avg
    dropout_ratio:      0.8
        
group: first_conv_weight has 1 params, lr_mult: 1, decay_mult: 1
group: first_conv_bias has 0 params, lr_mult: 2, decay_mult: 0
group: normal_weight has 20 params, lr_mult: 1, decay_mult: 1
group: normal_bias has 1 params, lr_mult: 2, decay_mult: 0
group: BN scale/shift has 2 params, lr_mult: 1, decay_mult: 0
Freezing BatchNorm2D except the first one.
/lfs01/workdirs/alex039/alex039u2/tsn_paper/server_scripts/faze_tsn_pytorch/models.py:75: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  normal(self.new_fc.weight, 0, std)
/lfs01/workdirs/alex039/alex039u2/tsn_paper/server_scripts/faze_tsn_pytorch/models.py:76: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  constant(self.new_fc.bias, 0)
/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:187: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.
  warnings.warn("The use of the transforms.Scale transform is deprecated, " +
Epoch: [0][0/191], lr: 0.00100	Time 9.642 (9.642)	Data 3.521 (3.521)	Loss 4.6102 (4.6102)	Prec@1 0.000 (0.000)	Prec@5 2.000 (2.000)
Epoch: [0][20/191], lr: 0.00100	Time 0.501 (0.915)	Data 0.000 (0.168)	Loss 4.5454 (4.5822)	Prec@1 2.000 (2.571)	Prec@5 12.000 (8.952)
Epoch: [0][40/191], lr: 0.00100	Time 0.552 (0.742)	Data 0.000 (0.087)	Loss 4.3997 (4.5271)	Prec@1 12.000 (3.366)	Prec@5 26.000 (12.976)
Epoch: [0][60/191], lr: 0.00100	Time 3.892 (0.817)	Data 3.163 (0.178)	Loss 4.0980 (4.4394)	Prec@1 12.000 (5.574)	Prec@5 28.000 (17.738)
Epoch: [0][80/191], lr: 0.00100	Time 0.637 (0.935)	Data 0.043 (0.285)	Loss 3.3139 (4.2726)	Prec@1 20.000 (8.667)	Prec@5 52.000 (23.531)
Epoch: [0][100/191], lr: 0.00100	Time 5.572 (1.134)	Data 4.794 (0.472)	Loss 2.8901 (4.0478)	Prec@1 28.000 (11.901)	Prec@5 60.000 (29.228)
Epoch: [0][120/191], lr: 0.00100	Time 0.951 (1.238)	Data 0.000 (0.558)	Loss 2.6923 (3.8289)	Prec@1 34.000 (15.388)	Prec@5 64.000 (34.661)
Epoch: [0][140/191], lr: 0.00100	Time 10.589 (1.463)	Data 9.609 (0.727)	Loss 2.3142 (3.6418)	Prec@1 42.000 (18.312)	Prec@5 66.000 (39.206)
Epoch: [0][160/191], lr: 0.00100	Time 0.714 (1.613)	Data 0.000 (0.864)	Loss 2.1266 (3.4775)	Prec@1 46.000 (20.994)	Prec@5 68.000 (42.919)
Epoch: [0][180/191], lr: 0.00100	Time 17.794 (1.807)	Data 16.740 (1.052)	Loss 2.2391 (3.3500)	Prec@1 40.000 (23.116)	Prec@5 76.000 (46.022)
Freezing BatchNorm2D except the first one.
main.py:182: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  total_norm = clip_grad_norm(model.parameters(), args.clip_gradient)
Epoch: [1][0/191], lr: 0.00100	Time 58.648 (58.648)	Data 56.317 (56.317)	Loss 1.3991 (1.3991)	Prec@1 68.000 (68.000)	Prec@5 88.000 (88.000)
Epoch: [1][20/191], lr: 0.00100	Time 1.017 (5.548)	Data 0.000 (4.542)	Loss 2.0319 (2.0776)	Prec@1 48.000 (45.810)	Prec@5 72.000 (75.619)
Epoch: [1][40/191], lr: 0.00100	Time 25.878 (5.012)	Data 24.613 (4.002)	Loss 2.1025 (1.9859)	Prec@1 46.000 (48.146)	Prec@5 76.000 (77.122)
Epoch: [1][60/191], lr: 0.00100	Time 1.147 (4.618)	Data 0.000 (3.550)	Loss 1.4130 (1.9310)	Prec@1 68.000 (49.180)	Prec@5 88.000 (78.590)
Epoch: [1][80/191], lr: 0.00100	Time 34.257 (4.872)	Data 32.584 (3.745)	Loss 1.8396 (1.9039)	Prec@1 42.000 (49.951)	Prec@5 78.000 (79.136)
Epoch: [1][100/191], lr: 0.00100	Time 3.655 (4.780)	Data 1.794 (3.597)	Loss 1.6632 (1.8762)	Prec@1 54.000 (50.158)	Prec@5 78.000 (79.485)
Epoch: [1][120/191], lr: 0.00100	Time 35.452 (5.085)	Data 33.531 (3.851)	Loss 1.4899 (1.8569)	Prec@1 60.000 (50.463)	Prec@5 80.000 (79.868)
Epoch: [1][140/191], lr: 0.00100	Time 2.737 (5.085)	Data 0.878 (3.808)	Loss 1.7340 (1.8282)	Prec@1 50.000 (50.766)	Prec@5 80.000 (80.567)
Epoch: [1][160/191], lr: 0.00100	Time 44.545 (5.408)	Data 42.397 (4.083)	Loss 1.8875 (1.7928)	Prec@1 60.000 (51.665)	Prec@5 82.000 (81.193)
Epoch: [1][180/191], lr: 0.00100	Time 3.327 (5.422)	Data 1.084 (4.050)	Loss 1.6299 (1.7669)	Prec@1 54.000 (52.486)	Prec@5 82.000 (81.514)
Freezing BatchNorm2D except the first one.
Epoch: [2][0/191], lr: 0.00100	Time 79.271 (79.271)	Data 76.906 (76.906)	Loss 1.3993 (1.3993)	Prec@1 60.000 (60.000)	Prec@5 84.000 (84.000)
Epoch: [2][20/191], lr: 0.00100	Time 3.183 (10.187)	Data 0.813 (8.008)	Loss 1.3569 (1.3791)	Prec@1 64.000 (61.905)	Prec@5 86.000 (87.905)
Epoch: [2][40/191], lr: 0.00100	Time 20.616 (8.802)	Data 18.563 (6.726)	Loss 1.5172 (1.4223)	Prec@1 60.000 (60.488)	Prec@5 84.000 (87.610)
Epoch: [2][60/191], lr: 0.00100	Time 1.778 (8.661)	Data 0.000 (6.531)	Loss 1.6482 (1.4394)	Prec@1 54.000 (59.738)	Prec@5 88.000 (87.508)
Epoch: [2][80/191], lr: 0.00100	Time 22.759 (8.558)	Data 20.186 (6.449)	Loss 1.2966 (1.4387)	Prec@1 62.000 (59.877)	Prec@5 88.000 (87.383)
Epoch: [2][100/191], lr: 0.00100	Time 1.704 (8.565)	Data 0.000 (6.458)	Loss 1.5758 (1.4422)	Prec@1 58.000 (59.525)	Prec@5 82.000 (87.485)
Epoch: [2][120/191], lr: 0.00100	Time 24.670 (8.716)	Data 21.519 (6.581)	Loss 0.9077 (1.4268)	Prec@1 78.000 (59.901)	Prec@5 96.000 (87.669)
Epoch: [2][140/191], lr: 0.00100	Time 2.271 (8.778)	Data 0.000 (6.620)	Loss 0.8515 (1.4062)	Prec@1 80.000 (60.525)	Prec@5 96.000 (87.901)
Epoch: [2][160/191], lr: 0.00100	Time 26.524 (8.838)	Data 24.316 (6.654)	Loss 1.0652 (1.3955)	Prec@1 70.000 (60.720)	Prec@5 92.000 (88.099)
Epoch: [2][180/191], lr: 0.00100	Time 2.470 (8.949)	Data 0.000 (6.738)	Loss 1.3554 (1.3751)	Prec@1 60.000 (61.436)	Prec@5 88.000 (88.387)
Freezing BatchNorm2D except the first one.
Epoch: [3][0/191], lr: 0.00100	Time 143.144 (143.144)	Data 138.344 (138.344)	Loss 1.2549 (1.2549)	Prec@1 58.000 (58.000)	Prec@5 88.000 (88.000)
Epoch: [3][20/191], lr: 0.00100	Time 2.120 (16.270)	Data 0.000 (13.307)	Loss 0.9383 (1.1282)	Prec@1 72.000 (68.762)	Prec@5 96.000 (91.619)
Epoch: [3][40/191], lr: 0.00100	Time 62.068 (14.442)	Data 57.879 (11.542)	Loss 0.5629 (1.1251)	Prec@1 84.000 (68.683)	Prec@5 96.000 (91.659)
Epoch: [3][60/191], lr: 0.00100	Time 2.387 (13.094)	Data 0.000 (10.210)	Loss 1.1766 (1.1506)	Prec@1 66.000 (67.574)	Prec@5 94.000 (91.672)
Epoch: [3][80/191], lr: 0.00100	Time 56.730 (13.102)	Data 52.895 (10.240)	Loss 1.3397 (1.1510)	Prec@1 56.000 (67.605)	Prec@5 86.000 (91.704)
Epoch: [3][100/191], lr: 0.00100	Time 3.029 (12.795)	Data 0.000 (9.884)	Loss 0.9328 (1.1539)	Prec@1 70.000 (67.010)	Prec@5 94.000 (91.703)
Epoch: [3][120/191], lr: 0.00100	Time 55.540 (12.733)	Data 52.246 (9.785)	Loss 1.2364 (1.1368)	Prec@1 64.000 (67.174)	Prec@5 92.000 (91.950)
Epoch: [3][140/191], lr: 0.00100	Time 2.663 (12.790)	Data 0.000 (9.762)	Loss 1.1913 (1.1245)	Prec@1 64.000 (67.404)	Prec@5 90.000 (92.014)
Epoch: [3][160/191], lr: 0.00100	Time 71.105 (12.973)	Data 67.197 (9.955)	Loss 1.1395 (1.1181)	Prec@1 66.000 (67.739)	Prec@5 92.000 (92.124)
Epoch: [3][180/191], lr: 0.00100	Time 3.359 (13.022)	Data 0.000 (9.964)	Loss 1.1472 (1.1252)	Prec@1 68.000 (67.547)	Prec@5 92.000 (92.011)
Freezing BatchNorm2D except the first one.
Epoch: [4][0/191], lr: 0.00100	Time 166.668 (166.668)	Data 161.686 (161.686)	Loss 1.6771 (1.6771)	Prec@1 58.000 (58.000)	Prec@5 80.000 (80.000)
Epoch: [4][20/191], lr: 0.00100	Time 3.217 (20.535)	Data 0.000 (16.766)	Loss 1.2164 (0.9872)	Prec@1 64.000 (72.095)	Prec@5 90.000 (92.571)
Epoch: [4][40/191], lr: 0.00100	Time 67.929 (17.117)	Data 63.470 (13.547)	Loss 1.1098 (0.9876)	Prec@1 68.000 (72.878)	Prec@5 96.000 (92.732)
Epoch: [4][60/191], lr: 0.00100	Time 3.158 (15.513)	Data 0.000 (11.882)	Loss 1.0278 (0.9679)	Prec@1 72.000 (73.246)	Prec@5 92.000 (92.885)
Epoch: [4][80/191], lr: 0.00100	Time 63.395 (15.632)	Data 58.799 (11.995)	Loss 1.0984 (0.9695)	Prec@1 64.000 (73.062)	Prec@5 96.000 (92.938)
Epoch: [4][100/191], lr: 0.00100	Time 3.585 (15.330)	Data 0.000 (11.687)	Loss 0.6431 (0.9719)	Prec@1 88.000 (72.515)	Prec@5 96.000 (93.050)
Epoch: [4][120/191], lr: 0.00100	Time 64.994 (15.566)	Data 59.599 (11.850)	Loss 1.2161 (0.9771)	Prec@1 62.000 (72.231)	Prec@5 90.000 (93.041)
Epoch: [4][140/191], lr: 0.00100	Time 3.571 (15.485)	Data 0.000 (11.693)	Loss 0.8321 (0.9680)	Prec@1 80.000 (72.468)	Prec@5 92.000 (93.191)
Epoch: [4][160/191], lr: 0.00100	Time 87.185 (15.751)	Data 81.994 (11.930)	Loss 1.2211 (0.9747)	Prec@1 68.000 (72.062)	Prec@5 88.000 (93.093)
Epoch: [4][180/191], lr: 0.00100	Time 3.992 (15.766)	Data 0.000 (11.906)	Loss 0.9680 (0.9713)	Prec@1 74.000 (72.055)	Prec@5 96.000 (93.249)
Freezing BatchNorm2D except the first one.
Test: [0/76]	Time 219.020 (219.020)	Loss 1.7530 (1.7530)	Prec@1 34.000 (34.000)	Prec@5 94.000 (94.000)
Test: [20/76]	Time 18.663 (25.539)	Loss 0.0480 (0.9491)	Prec@1 100.000 (71.524)	Prec@5 100.000 (94.095)
Test: [40/76]	Time 94.218 (21.477)	Loss 1.6271 (1.1578)	Prec@1 60.000 (65.902)	Prec@5 88.000 (90.927)
Test: [60/76]	Time 2.778 (19.155)	Loss 0.4965 (1.0869)	Prec@1 82.000 (68.656)	Prec@5 100.000 (90.984)
Testing Results: Prec@1 69.707 Prec@5 91.118 Loss 1.07089
Freezing BatchNorm2D except the first one.
main.py:215: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  input_var = torch.autograd.Variable(input, volatile=True)
main.py:216: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  target_var = torch.autograd.Variable(target, volatile=True)
Epoch: [5][0/191], lr: 0.00100	Time 268.279 (268.279)	Data 259.405 (259.405)	Loss 0.8604 (0.8604)	Prec@1 72.000 (72.000)	Prec@5 94.000 (94.000)
Epoch: [5][20/191], lr: 0.00100	Time 6.026 (27.569)	Data 0.000 (22.451)	Loss 1.0151 (0.8606)	Prec@1 68.000 (73.429)	Prec@5 88.000 (93.714)
Epoch: [5][40/191], lr: 0.00100	Time 114.450 (25.310)	Data 109.016 (20.268)	Loss 1.0925 (0.8526)	Prec@1 66.000 (73.707)	Prec@5 92.000 (94.439)
Epoch: [5][60/191], lr: 0.00100	Time 4.166 (22.324)	Data 0.000 (17.420)	Loss 0.5552 (0.8576)	Prec@1 86.000 (73.672)	Prec@5 96.000 (94.426)
Epoch: [5][80/191], lr: 0.00100	Time 131.249 (22.881)	Data 126.446 (18.071)	Loss 0.9632 (0.8649)	Prec@1 78.000 (73.556)	Prec@5 94.000 (94.395)
Epoch: [5][100/191], lr: 0.00100	Time 4.654 (21.688)	Data 0.000 (16.843)	Loss 0.8621 (0.8722)	Prec@1 78.000 (73.545)	Prec@5 96.000 (94.436)
Epoch: [5][120/191], lr: 0.00100	Time 147.854 (22.129)	Data 138.519 (17.228)	Loss 0.8477 (0.8697)	Prec@1 70.000 (73.636)	Prec@5 96.000 (94.496)
Epoch: [5][140/191], lr: 0.00100	Time 4.713 (21.414)	Data 0.000 (16.503)	Loss 0.8746 (0.8837)	Prec@1 74.000 (73.475)	Prec@5 94.000 (94.369)
Epoch: [5][160/191], lr: 0.00100	Time 144.601 (21.783)	Data 137.542 (16.823)	Loss 0.8098 (0.8770)	Prec@1 70.000 (73.689)	Prec@5 94.000 (94.497)
Epoch: [5][180/191], lr: 0.00100	Time 4.837 (21.595)	Data 0.000 (16.617)	Loss 0.7087 (0.8783)	Prec@1 80.000 (73.724)	Prec@5 98.000 (94.508)
Freezing BatchNorm2D except the first one.
