{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this part, we will modify our CNN architecture whether it is ResNet, VGG, ..etc to cope with the new architecture.\n",
    "\n",
    "Architectures usually have some constrains like the input image for example should be 224*224 and so on, so we will have to construct our own models by modifying the main CNN architecture. Mainly, we will modify two main part, the train part which is responsible for training our weights and the RGB Difference model to build more channels for more concatenated images.\n",
    "\n",
    "First, let's import some important libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.nn.init import normal, constant\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "#from ops.basic_ops import ConsensusModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will build our TSN class which almost has everything ready to get started. We will prepare our base model (vgg, resnet, ..etc) and edit its last layer with our number of classes for actions (e.g. 101 for UCF101 dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSN_model(nn.Module):                                             #nn.Moudle is a base class, the model class should subclass this one\n",
    "  \n",
    "    def __init__ (self, num_classes, num_segments, modality, consensus_type='avg', base_model_name='resnet18',\n",
    "                 new_length=None, before_softmax=True, dropout=0.8, crop_num=1, partial_bn=True):\n",
    "    \n",
    "        super(TSN_model, self).__init__()                                               #Excute all nn.Moudle __init__ fuction stuff before anything as a base class.\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_segments = num_segments\n",
    "        self.modality = modality\n",
    "        self.base_model_name = base_model_name\n",
    "        self.consensus_type = consensus_type\n",
    "        self.before_softmax = before_softmax                                              ####\n",
    "        self.dropout = dropout\n",
    "        self.crop_num = crop_num                                                    ####\n",
    "        self.reshape = True                                                        ####\n",
    "        self.partial_bn = partial_bn\n",
    "\n",
    "        if not before_softmax and consensus_type != 'avg':                                 ####\n",
    "            raise ValueError(\"Only avg consensus can be used after Softmax\")\n",
    "\n",
    "        if new_length is None:                                               #Setting the number of frames picked from each segments \n",
    "            self.new_length = 1 if modality == \"RGB\" else 5\n",
    "        else:\n",
    "            self.new_length = new_length\n",
    "        \n",
    "        self.prepare_model(base_model_name, self.num_classes)\n",
    "       # self.consensus = ConsensusModule(consensus_type)                    #Creating Consensus layer (Only 'avg' and 'identity' is available)\n",
    "\n",
    "        print((\"\"\"\n",
    "                Initializing TSN with base model: {}.\n",
    "                TSN Configurations:\n",
    "                    input_modality:     {}\n",
    "                    num_segments:       {}\n",
    "                    new_length:         {}\n",
    "                    consensus_module:   {}\n",
    "                    dropout_ratio:      {}\n",
    "               \"\"\".format(base_model_name, self.modality, self.num_segments, self.new_length, self.consensus_type, self.dropout)))\n",
    "\n",
    "\n",
    "        if self.modality == 'RGBDiff':\n",
    "            print(\"Converting the ImageNet model to RGBDiff model\")\n",
    "            self.base_model = self.construct_diff_model(self.base_model)\n",
    "            print(\"Done. RGBDiff model is ready.\")\n",
    "\n",
    "\n",
    "        if not self.before_softmax:                                         #Creating softmax Layer if necessary\n",
    "            self.softmax = nn.Softmax()\n",
    "            \n",
    "    #this function is used to modify the last layer (fully connected layer) for a given architecture to suit our dataset number of actions\n",
    "    def prepare_model(self, base_model_name, num_classes):\n",
    "        \"\"\"\n",
    "        base_model: string contains the model name \n",
    "        This function get the base model from torchvision pretrained models and set some variables according to the input model name\n",
    "        \"\"\"\n",
    "        #add other architectures later\n",
    "        if 'resnet' in base_model_name:\n",
    "            self.base_model = getattr(torchvision.models, base_model_name)(pretrained=True)   #Load pretrained model\n",
    "            self.last_layer_name = 'fc'\n",
    "            self.input_size = 224                                                                #set the input size for the model\n",
    "            self.input_mean = [0.485, 0.456, 0.406]                                              #set 3 channel mean values (standard values) for normalization\n",
    "            self.input_std = [0.229, 0.224, 0.225]                                               #set 3 chaneel standard deviation values for normalization\n",
    "\n",
    "            #There's no point of substarct means from RGBDiff frames\n",
    "            if self.modality == 'RGBDiff':\n",
    "                self.input_mean = [0.485, 0.456, 0.406] + [0] * 3 * self.new_length                      #[0.485, 0.456, 0.406 , 0, 0, 0, 0, 0,.....]\n",
    "                self.input_std = self.input_std + [np.mean(self.input_std) * 2] * 3 * self.new_length    #Expand the list with the average 0.452   \n",
    "\n",
    "        else:\n",
    "            raise ValueError('Unknown base model: {}'.format(base_model_name))\n",
    "\n",
    "        features_dim = getattr(self.base_model, self.last_layer_name).in_features\n",
    "\n",
    "        if self.dropout == 0:\n",
    "            setattr(self.base_model, self.last_layer_name, nn.Linear(features_dim, num_classes))\n",
    "            self.new_fc = getattr(self.base_model, self.last_layer_name)\n",
    "\n",
    "        else:\n",
    "            setattr(self.base_model, self.last_layer_name, nn.Dropout(self.dropout))\n",
    "            self.new_fc = nn.Linear(features_dim, num_classes)\n",
    "        \n",
    "        print(self.new_fc)\n",
    "\n",
    "        std=0.001\n",
    "        normal(self.new_fc.weight, 0, std)\n",
    "        constant(self.new_fc.bias,0)\n",
    "        \n",
    "        print(self.new_fc.bias)\n",
    "        #what is normal and constant used for ??????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try creating an object and manipulate it to make sure it is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=512, out_features=101, bias=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "\n",
      "                Initializing TSN with base model: resnet18.\n",
      "                TSN Configurations:\n",
      "                    input_modality:     RGBDiff\n",
      "                    num_segments:       3\n",
      "                    new_length:         5\n",
      "                    consensus_module:   avg\n",
      "                    dropout_ratio:      0.8\n",
      "\n",
      "               \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:92: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    }
   ],
   "source": [
    "obj = TSN_model(num_classes=101, num_segments=3, modality='RGBDiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Dropout(p=0.8)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to be modified is the train section. We should set Batch Normalization layer to ve freezed except the first one for smooth training.\n",
    "For any more details, please refer to this paper: https://arxiv.org/pdf/1502.03167.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, mode=True):\n",
    "    \"\"\"\n",
    "    this function freezes batch normalization layers except the first one.\n",
    "    inputs: mode (True for the training process)\n",
    "    \"\"\"\n",
    "    super(TSN, self).train(mode)\n",
    "    count=0\n",
    "    \n",
    "    #check if partial batch normalization is activated\n",
    "    if self.partial_bn:\n",
    "        for m in self.base_model.modules():\n",
    "            if isinstance(m, nnBatchNorm2d):\n",
    "                #freeze the layers except the first one\n",
    "                if not self.partial_bn or count > 0:\n",
    "                    m.eval()  \n",
    "                    m.weight.requires_grad = False\n",
    "                    m.bias.requires_grad = False\n",
    "                else:\n",
    "                    count=+1\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function for subtracting our frames to obtain our RGB difference model. the length of frames for each segment is usually set to be 5 (you can change it but this gave the best accuracy in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " def extract_rgbDiff(self,RGB_tensor,keep_rgb=False):\n",
    "    \"\"\"\n",
    "    RGB_tensor : Tensor contian all frames picked from 1 Video --Size(Number of frames,3,H,W)\n",
    "    keep_rgb   : Boolean True(Keep an RGB frame [RGB, RGBDiff, RGBDiff, RGBDiff....])\n",
    "                        False(All frames are RGBDiff)\n",
    "    \"\"\"\n",
    "    #Reshape the tensor to (1 , Num of segments , Number of picked frames , Channels , Hight , Width)\n",
    "    RGB_tensor = RGB_tensor.view((-1 , self.num_segments , self.new_length+1 , 3 ) + RGB_tensor.size()[2:])\n",
    "\n",
    "    if keep_rgb:\n",
    "        RGBDiff_tensor= RGB_tensor.clone()\n",
    "    else:\n",
    "        RGBDiff_tensor = RGB_tensor[:, :, 1:, :, :, :].clone()\n",
    "\n",
    "    #Generate RGBDiff frames\n",
    "    #if keep_rgb is set to True, then we will use two streams, one for RGB and one for RGB Diff, so we have to leave the first frame\n",
    "    #in RGB_tensor non-subtracted\n",
    "    for x in reversed(list(range(1, self.new_length + 1))):\n",
    "        if keep_rgb:\n",
    "            RGBDiff_tensor[:, :, x, :, :, :]     = RGB_tensor[:, :, x, :, :, :] - RGB_tensor[:, :, x - 1, :, :, :]\n",
    "        else:\n",
    "            RGBDiff_tensor[:, :, x - 1, :, :, :] = RGB_tensor[:, :, x, :, :, :] - RGB_tensor[:, :, x - 1, :, :, :]\n",
    "\n",
    "    return RGBDiff_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is alright up till now. The one last thing to do is to update our first conv2d layer with the appropriate number of channels to be suited for the number of frames for each segment which will go through the CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_rgbDiff(self, base_model, keep_rgb=True):\n",
    "    \n",
    "    modules = list(self.base_model.modules())\n",
    "    \n",
    "    #check the index for the first conv2d layer\n",
    "    for i in range(len(modules)):\n",
    "        if isinstance(modules[i], nn.Conv2d):\n",
    "            first_conv_idx = i\n",
    "            break\n",
    "        \n",
    "    conv_layer = modules[first_conv_idx]\n",
    "    container = modules[first_conv_idx-1]\n",
    "    \n",
    "    params = [x.clone() for x in conv_layer.parameters()]\n",
    "    kernel_size = params[0].size()\n",
    "    \n",
    "    #to be continued after final exams ----------------------\n",
    "    if not keep_rgb:\n",
    "        new_kernel_size = kernel_size[:1] + (3 * self.new_length,) + kernel_size[2:]\n",
    "        new_kernels = params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()\n",
    "    else:\n",
    "        new_kernel_size = kernel_size[:1] + (3 * self.new_length,) + kernel_size[2:]\n",
    "        new_kernels = torch.cat((params[0].data, \n",
    "                                 params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()),1)\n",
    "        new_kernel_size = new_kernels.size()\n",
    "        \n",
    "    new_conv = nn.Conv2d(new_kernel_size[1], conv_layer.out_channels, conv_layer.kernel_size,\n",
    "                         conv_layer.stride, conv_layer.padding, bias=True if len(params)==2 else False)\n",
    "    \n",
    "    new_conv.weight.data = new_kernels\n",
    "    if len(params) == 2:\n",
    "        new_conv.bias.data = params[1].data  # add bias if neccessary\n",
    "    layer_name = list(container.state_dict().keys())[0][:-7]  # remove .weight suffix to get the layer name\n",
    "    \n",
    "    # replace the first convolution layer\n",
    "    setattr(container, layer_name, new_conv)\n",
    "    return base_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
