/var/spool/slurmd/job00108/slurm_script: /usr/share/lmod/lmod/libexec/lmod: No such file or directory
Start Training ...................................

          Initializing TSN with base model: BNInception.
          TSN Configurations:
              input_modality:     RGBDiff
              num_segments:       7
              new_length:         5
              consensus_module:   avg
              dropout_ratio:      0.8
               
Load and modify the standard model FC output layer
Dropout Layer added and The modified linear layer is : Linear(in_features=1024, out_features=101, bias=True)
Done. Loading and Modifying 
 ---------------------------------------------------
Converting the ImageNet model to RGBDiff model
The modified 1st layer is Conv2d(15, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))
Done. RGBDiff model is ready.
---------------------------------------------------
group: first_conv_weight has 1 params, lr_mult: 1, decay_mult: 1
group: first_conv_bias has 1 params, lr_mult: 2, decay_mult: 0
group: normal_weight has 69 params, lr_mult: 1, decay_mult: 1
group: normal_bias has 69 params, lr_mult: 2, decay_mult: 0
group: BN scale/shift has 2 params, lr_mult: 1, decay_mult: 0
---------------------------------------------------
Epoch: [0][0/150], lr: 0.00100	Time 109.144 (109.144)	Data 51.099 (51.099)	Loss 4.6104 (4.6104)	Acc@1 0.000 (0.000)	Acc@5 1.562 (1.562)
Epoch: [0][20/150], lr: 0.00100	Time 3.608 (9.032)	Data 0.002 (2.436)	Loss 4.4632 (4.5564)	Acc@1 6.250 (3.199)	Acc@5 10.938 (10.342)
Epoch: [0][40/150], lr: 0.00100	Time 3.674 (6.515)	Data 0.006 (1.249)	Loss 3.9403 (4.3737)	Acc@1 10.938 (5.335)	Acc@5 23.438 (16.921)
Epoch: [0][60/150], lr: 0.00100	Time 4.446 (5.679)	Data 0.000 (0.840)	Loss 3.4906 (4.1368)	Acc@1 20.312 (8.376)	Acc@5 51.562 (24.078)
Epoch: [0][80/150], lr: 0.00100	Time 3.101 (5.223)	Data 0.000 (0.633)	Loss 3.1285 (3.9032)	Acc@1 12.500 (12.114)	Acc@5 59.375 (30.845)
Epoch: [0][100/150], lr: 0.00100	Time 3.290 (4.966)	Data 0.000 (0.508)	Loss 2.7425 (3.6968)	Acc@1 28.125 (15.130)	Acc@5 57.812 (36.386)
Epoch: [0][120/150], lr: 0.00100	Time 4.588 (4.800)	Data 0.000 (0.424)	Loss 2.0828 (3.5036)	Acc@1 48.438 (18.879)	Acc@5 75.000 (41.490)
Epoch: [0][140/150], lr: 0.00100	Time 2.913 (4.644)	Data 0.001 (0.364)	Loss 2.1436 (3.3288)	Acc@1 46.875 (22.207)	Acc@5 76.562 (45.878)
Epoch: [1][0/150], lr: 0.00100	Time 52.238 (52.238)	Data 46.870 (46.870)	Loss 1.7903 (1.7903)	Acc@1 46.875 (46.875)	Acc@5 82.812 (82.812)
Epoch: [1][20/150], lr: 0.00100	Time 3.136 (6.654)	Data 0.001 (2.621)	Loss 1.8273 (1.9397)	Acc@1 50.000 (47.545)	Acc@5 85.938 (78.795)
Epoch: [1][40/150], lr: 0.00100	Time 3.366 (5.356)	Data 0.000 (1.343)	Loss 1.6283 (1.8547)	Acc@1 57.812 (50.724)	Acc@5 81.250 (80.526)
Epoch: [1][60/150], lr: 0.00100	Time 5.823 (4.938)	Data 0.000 (0.903)	Loss 1.5978 (1.7901)	Acc@1 57.812 (52.408)	Acc@5 85.938 (81.455)
Epoch: [1][80/150], lr: 0.00100	Time 3.311 (4.693)	Data 0.000 (0.681)	Loss 1.5596 (1.7443)	Acc@1 50.000 (53.395)	Acc@5 90.625 (82.253)
Epoch: [1][100/150], lr: 0.00100	Time 3.870 (4.563)	Data 0.003 (0.546)	Loss 1.5366 (1.7009)	Acc@1 53.125 (54.069)	Acc@5 84.375 (83.045)
Epoch: [1][120/150], lr: 0.00100	Time 5.653 (4.468)	Data 0.001 (0.456)	Loss 1.8328 (1.6443)	Acc@1 51.562 (55.127)	Acc@5 84.375 (84.168)
Epoch: [1][140/150], lr: 0.00100	Time 2.914 (4.366)	Data 0.000 (0.392)	Loss 1.5243 (1.6098)	Acc@1 57.812 (56.006)	Acc@5 79.688 (84.563)
Epoch: [2][0/150], lr: 0.00100	Time 58.284 (58.284)	Data 53.798 (53.798)	Loss 1.2060 (1.2060)	Acc@1 65.625 (65.625)	Acc@5 93.750 (93.750)
Epoch: [2][20/150], lr: 0.00100	Time 3.694 (7.273)	Data 0.001 (3.207)	Loss 0.9623 (1.2876)	Acc@1 71.875 (64.062)	Acc@5 90.625 (90.179)
Epoch: [2][40/150], lr: 0.00100	Time 5.713 (6.164)	Data 0.007 (2.100)	Loss 0.8863 (1.2164)	Acc@1 73.438 (65.282)	Acc@5 92.188 (91.159)
Epoch: [2][60/150], lr: 0.00100	Time 3.183 (5.609)	Data 0.001 (1.518)	Loss 0.9610 (1.1591)	Acc@1 67.188 (67.008)	Acc@5 98.438 (91.675)
Epoch: [2][80/150], lr: 0.00100	Time 3.375 (5.206)	Data 0.001 (1.144)	Loss 0.9395 (1.1490)	Acc@1 65.625 (67.323)	Acc@5 95.312 (91.628)
Epoch: [2][100/150], lr: 0.00100	Time 4.998 (4.964)	Data 0.000 (0.917)	Loss 1.2265 (1.1183)	Acc@1 67.188 (68.131)	Acc@5 92.188 (92.002)
Epoch: [2][120/150], lr: 0.00100	Time 3.434 (4.781)	Data 0.001 (0.766)	Loss 0.8401 (1.0986)	Acc@1 71.875 (68.724)	Acc@5 98.438 (92.278)
Epoch: [2][140/150], lr: 0.00100	Time 3.333 (4.631)	Data 0.000 (0.657)	Loss 1.2275 (1.0801)	Acc@1 64.062 (69.171)	Acc@5 93.750 (92.453)
Epoch: [3][0/150], lr: 0.00100	Time 57.590 (57.590)	Data 52.421 (52.421)	Loss 0.8924 (0.8924)	Acc@1 73.438 (73.438)	Acc@5 93.750 (93.750)
Epoch: [3][20/150], lr: 0.00100	Time 4.863 (7.048)	Data 0.002 (2.932)	Loss 0.9147 (1.0090)	Acc@1 76.562 (71.875)	Acc@5 90.625 (92.560)
Epoch: [3][40/150], lr: 0.00100	Time 3.032 (5.596)	Data 0.003 (1.503)	Loss 1.0560 (0.9430)	Acc@1 64.062 (73.095)	Acc@5 92.188 (93.369)
Epoch: [3][60/150], lr: 0.00100	Time 3.923 (5.082)	Data 0.000 (1.011)	Loss 0.8276 (0.9007)	Acc@1 78.125 (74.155)	Acc@5 95.312 (93.878)
Epoch: [3][80/150], lr: 0.00100	Time 5.011 (4.783)	Data 0.001 (0.761)	Loss 0.8621 (0.8853)	Acc@1 76.562 (74.653)	Acc@5 93.750 (94.155)
Epoch: [3][100/150], lr: 0.00100	Time 2.941 (4.635)	Data 0.000 (0.611)	Loss 0.7529 (0.8638)	Acc@1 81.250 (75.232)	Acc@5 95.312 (94.462)
Epoch: [3][120/150], lr: 0.00100	Time 3.384 (4.536)	Data 0.001 (0.511)	Loss 1.0367 (0.8723)	Acc@1 71.875 (75.000)	Acc@5 95.312 (94.421)
Epoch: [3][140/150], lr: 0.00100	Time 4.550 (4.419)	Data 0.000 (0.438)	Loss 0.8744 (0.8578)	Acc@1 71.875 (75.244)	Acc@5 90.625 (94.648)
Epoch: [4][0/150], lr: 0.00100	Time 177.705 (177.705)	Data 159.788 (159.788)	Loss 0.7836 (0.7836)	Acc@1 75.000 (75.000)	Acc@5 96.875 (96.875)
Epoch: [4][20/150], lr: 0.00100	Time 3.332 (14.554)	Data 0.000 (9.649)	Loss 0.9643 (0.7856)	Acc@1 71.875 (77.381)	Acc@5 90.625 (94.420)
Epoch: [4][40/150], lr: 0.00100	Time 3.744 (10.007)	Data 0.008 (5.493)	Loss 0.7445 (0.7778)	Acc@1 76.562 (77.096)	Acc@5 96.875 (95.122)
Epoch: [4][60/150], lr: 0.00100	Time 4.509 (8.322)	Data 0.001 (3.901)	Loss 0.6360 (0.7697)	Acc@1 79.688 (77.254)	Acc@5 96.875 (95.441)
Epoch: [4][80/150], lr: 0.00100	Time 3.757 (8.052)	Data 0.003 (3.620)	Loss 0.3508 (0.7443)	Acc@1 87.500 (78.183)	Acc@5 98.438 (95.640)
Epoch: [4][100/150], lr: 0.00100	Time 4.225 (7.289)	Data 0.000 (2.928)	Loss 0.5833 (0.7237)	Acc@1 84.375 (78.744)	Acc@5 98.438 (95.854)
Epoch: [4][120/150], lr: 0.00100	Time 4.847 (6.901)	Data 0.000 (2.567)	Loss 0.5831 (0.7157)	Acc@1 85.938 (78.719)	Acc@5 96.875 (95.919)
Epoch: [4][140/150], lr: 0.00100	Time 77.447 (8.005)	Data 70.434 (3.584)	Loss 0.6863 (0.7018)	Acc@1 81.250 (79.178)	Acc@5 93.750 (96.166)
Test: [0/60]	Time 103.167 (103.167)	Loss 0.1867 (0.1867)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Test: [20/60]	Time 1.244 (7.957)	Loss 2.2977 (1.2520)	Acc@1 32.812 (67.485)	Acc@5 81.250 (88.765)
Test: [40/60]	Time 1.344 (5.478)	Loss 0.4969 (1.1522)	Acc@1 87.500 (68.864)	Acc@5 100.000 (90.206)
 * Acc@1 67.803 Acc@5 89.559 Loss 1.19344
Traceback (most recent call last):
  File "main.py", line 390, in <module>
    main()
  File "main.py", line 144, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File "main.py", line 206, in train
    output = model(input)
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 123, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 133, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 77, in parallel_apply
    raise output
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 53, in _worker
    output = module(*input, **kwargs)
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/share/lfs01/workdirs/alex039u2/tsn_paper/server_scripts/real-time-action-recognition/Modified_CNN.py", line 338, in forward
    FProp = self.base_model(input) 
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/share/lfs01/workdirs/alex039u2/tsn_paper/server_scripts/real-time-action-recognition/net/bn_inception.py", line 1300, in forward
    x = self.features(input)
  File "/share/lfs01/workdirs/alex039u2/tsn_paper/server_scripts/real-time-action-recognition/net/bn_inception.py", line 1289, in features
    1
RuntimeError: CUDA error: out of memory
End of Training .................................
