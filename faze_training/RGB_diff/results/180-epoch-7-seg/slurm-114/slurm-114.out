/var/spool/slurmd/job00114/slurm_script: /usr/share/lmod/lmod/libexec/lmod: No such file or directory
Start Training ...................................

          Initializing TSN with base model: BNInception.
          TSN Configurations:
              input_modality:     RGBDiff
              num_segments:       7
              new_length:         5
              consensus_module:   avg
              dropout_ratio:      0.8
               
Load and modify the standard model FC output layer
Dropout Layer added and The modified linear layer is : Linear(in_features=1024, out_features=101, bias=True)
Done. Loading and Modifying 
 ---------------------------------------------------
Converting the ImageNet model to RGBDiff model
The modified 1st layer is Conv2d(15, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))
Done. RGBDiff model is ready.
---------------------------------------------------
group: first_conv_weight has 1 params, lr_mult: 1, decay_mult: 1
group: first_conv_bias has 1 params, lr_mult: 2, decay_mult: 0
group: normal_weight has 69 params, lr_mult: 1, decay_mult: 1
group: normal_bias has 69 params, lr_mult: 2, decay_mult: 0
group: BN scale/shift has 2 params, lr_mult: 1, decay_mult: 0
---------------------------------------------------
Epoch: [0][0/150], lr: 0.00100	Time 155.684 (155.684)	Data 89.859 (89.859)	Loss 4.6240 (4.6240)	Acc@1 1.562 (1.562)	Acc@5 4.688 (4.688)
Epoch: [0][20/150], lr: 0.00100	Time 7.408 (12.829)	Data 0.000 (4.283)	Loss 4.4198 (4.5607)	Acc@1 1.562 (2.679)	Acc@5 17.188 (10.119)
Epoch: [0][40/150], lr: 0.00100	Time 6.426 (9.153)	Data 0.000 (2.194)	Loss 4.1311 (4.3845)	Acc@1 6.250 (4.916)	Acc@5 20.312 (16.616)
Epoch: [0][60/150], lr: 0.00100	Time 5.057 (7.873)	Data 0.000 (1.476)	Loss 3.5690 (4.1299)	Acc@1 17.188 (8.402)	Acc@5 43.750 (24.078)
Epoch: [0][80/150], lr: 0.00100	Time 5.385 (7.813)	Data 0.000 (1.553)	Loss 2.9984 (3.8949)	Acc@1 21.875 (12.249)	Acc@5 48.438 (30.883)
Epoch: [0][100/150], lr: 0.00100	Time 4.540 (7.871)	Data 0.000 (1.723)	Loss 2.7022 (3.6844)	Acc@1 29.688 (15.842)	Acc@5 59.375 (36.757)
Epoch: [0][120/150], lr: 0.00100	Time 4.914 (7.454)	Data 0.000 (1.438)	Loss 2.3182 (3.4739)	Acc@1 34.375 (19.357)	Acc@5 70.312 (42.420)
Epoch: [0][140/150], lr: 0.00100	Time 5.064 (7.140)	Data 0.000 (1.235)	Loss 2.1282 (3.3049)	Acc@1 46.875 (22.407)	Acc@5 81.250 (46.775)
Epoch: [1][0/150], lr: 0.00100	Time 57.986 (57.986)	Data 52.559 (52.559)	Loss 2.0173 (2.0173)	Acc@1 48.438 (48.438)	Acc@5 76.562 (76.562)
Epoch: [1][20/150], lr: 0.00100	Time 4.543 (7.728)	Data 0.000 (2.505)	Loss 1.7305 (1.9465)	Acc@1 54.688 (50.000)	Acc@5 81.250 (80.060)
Epoch: [1][40/150], lr: 0.00100	Time 4.794 (6.487)	Data 0.041 (1.285)	Loss 1.7625 (1.8947)	Acc@1 54.688 (51.029)	Acc@5 82.812 (80.183)
Epoch: [1][60/150], lr: 0.00100	Time 6.313 (6.060)	Data 0.000 (0.865)	Loss 1.5320 (1.8558)	Acc@1 56.250 (51.383)	Acc@5 82.812 (80.558)
Epoch: [1][80/150], lr: 0.00100	Time 4.984 (5.828)	Data 0.000 (0.651)	Loss 1.6837 (1.7770)	Acc@1 57.812 (53.106)	Acc@5 82.812 (81.752)
Epoch: [1][100/150], lr: 0.00100	Time 5.080 (5.699)	Data 0.000 (0.523)	Loss 1.2042 (1.7098)	Acc@1 68.750 (54.610)	Acc@5 92.188 (82.843)
Epoch: [1][120/150], lr: 0.00100	Time 4.864 (5.594)	Data 0.000 (0.437)	Loss 1.3247 (1.6656)	Acc@1 67.188 (55.372)	Acc@5 92.188 (83.665)
Epoch: [1][140/150], lr: 0.00100	Time 4.505 (5.525)	Data 0.000 (0.375)	Loss 1.6680 (1.6301)	Acc@1 56.250 (56.172)	Acc@5 87.500 (84.364)
Epoch: [2][0/150], lr: 0.00100	Time 57.924 (57.924)	Data 50.050 (50.050)	Loss 0.8773 (0.8773)	Acc@1 76.562 (76.562)	Acc@5 93.750 (93.750)
Epoch: [2][20/150], lr: 0.00100	Time 5.977 (7.876)	Data 0.001 (2.388)	Loss 1.2734 (1.2527)	Acc@1 65.625 (65.030)	Acc@5 89.062 (89.211)
Epoch: [2][40/150], lr: 0.00100	Time 4.880 (6.564)	Data 0.000 (1.223)	Loss 0.9055 (1.1979)	Acc@1 73.438 (65.320)	Acc@5 100.000 (90.434)
Epoch: [2][60/150], lr: 0.00100	Time 5.994 (6.118)	Data 0.007 (0.823)	Loss 0.9829 (1.2019)	Acc@1 75.000 (65.369)	Acc@5 95.312 (90.651)
Epoch: [2][80/150], lr: 0.00100	Time 4.872 (5.872)	Data 0.000 (0.620)	Loss 1.5816 (1.1879)	Acc@1 57.812 (66.107)	Acc@5 78.125 (90.471)
Epoch: [2][100/150], lr: 0.00100	Time 4.456 (5.735)	Data 0.000 (0.497)	Loss 0.8926 (1.1672)	Acc@1 75.000 (66.723)	Acc@5 96.875 (90.873)
Epoch: [2][120/150], lr: 0.00100	Time 4.843 (5.636)	Data 0.001 (0.415)	Loss 0.9792 (1.1508)	Acc@1 67.188 (67.200)	Acc@5 93.750 (91.064)
Epoch: [2][140/150], lr: 0.00100	Time 4.813 (5.546)	Data 0.000 (0.357)	Loss 1.1222 (1.1270)	Acc@1 68.750 (67.797)	Acc@5 89.062 (91.301)
Epoch: [3][0/150], lr: 0.00100	Time 62.316 (62.316)	Data 56.493 (56.493)	Loss 0.9105 (0.9105)	Acc@1 76.562 (76.562)	Acc@5 95.312 (95.312)
Epoch: [3][20/150], lr: 0.00100	Time 4.765 (7.977)	Data 0.000 (2.704)	Loss 0.7171 (0.9555)	Acc@1 78.125 (73.363)	Acc@5 96.875 (93.452)
Epoch: [3][40/150], lr: 0.00100	Time 4.841 (8.386)	Data 0.002 (2.988)	Loss 0.9693 (0.9443)	Acc@1 75.000 (73.514)	Acc@5 92.188 (93.445)
Epoch: [3][60/150], lr: 0.00100	Time 5.094 (7.776)	Data 0.001 (2.409)	Loss 0.9345 (0.9205)	Acc@1 70.312 (73.489)	Acc@5 98.438 (93.955)
Epoch: [3][80/150], lr: 0.00100	Time 4.497 (7.111)	Data 0.003 (1.815)	Loss 1.3580 (0.9161)	Acc@1 67.188 (73.708)	Acc@5 92.188 (93.866)
Epoch: [3][100/150], lr: 0.00100	Time 5.332 (6.739)	Data 0.015 (1.456)	Loss 1.1720 (0.9256)	Acc@1 73.438 (73.468)	Acc@5 85.938 (93.874)
Epoch: [3][120/150], lr: 0.00100	Time 4.529 (6.497)	Data 0.000 (1.216)	Loss 0.8350 (0.9072)	Acc@1 71.875 (73.915)	Acc@5 95.312 (94.112)
Epoch: [3][140/150], lr: 0.00100	Time 4.995 (6.325)	Data 0.000 (1.044)	Loss 1.0865 (0.8973)	Acc@1 71.875 (74.202)	Acc@5 89.062 (94.193)
Epoch: [4][0/150], lr: 0.00100	Time 55.259 (55.259)	Data 49.099 (49.099)	Loss 0.7451 (0.7451)	Acc@1 75.000 (75.000)	Acc@5 95.312 (95.312)
Epoch: [4][20/150], lr: 0.00100	Time 5.121 (8.054)	Data 0.000 (2.657)	Loss 0.8453 (0.6849)	Acc@1 73.438 (79.539)	Acc@5 93.750 (96.503)
Epoch: [4][40/150], lr: 0.00100	Time 6.874 (6.942)	Data 0.009 (1.547)	Loss 0.6980 (0.7030)	Acc@1 75.000 (78.582)	Acc@5 96.875 (96.303)
Epoch: [4][60/150], lr: 0.00100	Time 7.302 (6.500)	Data 2.197 (1.078)	Loss 0.3857 (0.7086)	Acc@1 85.938 (78.868)	Acc@5 100.000 (96.209)
Epoch: [4][80/150], lr: 0.00100	Time 5.223 (6.306)	Data 0.000 (0.877)	Loss 0.6909 (0.6796)	Acc@1 82.812 (79.668)	Acc@5 95.312 (96.219)
Epoch: [4][100/150], lr: 0.00100	Time 6.768 (6.117)	Data 0.002 (0.703)	Loss 0.5072 (0.6817)	Acc@1 85.938 (79.734)	Acc@5 95.312 (96.225)
Epoch: [4][120/150], lr: 0.00100	Time 53.459 (6.432)	Data 46.854 (0.975)	Loss 0.6675 (0.6753)	Acc@1 78.125 (80.062)	Acc@5 96.875 (96.307)
Epoch: [4][140/150], lr: 0.00100	Time 4.846 (6.352)	Data 0.001 (0.902)	Loss 0.4108 (0.6636)	Acc@1 87.500 (80.463)	Acc@5 98.438 (96.454)
Test: [0/60]	Time 79.721 (79.721)	Loss 0.7576 (0.7576)	Acc@1 73.438 (73.438)	Acc@5 98.438 (98.438)
Test: [20/60]	Time 1.078 (6.892)	Loss 2.2948 (0.9674)	Acc@1 53.125 (73.214)	Acc@5 71.875 (93.304)
Test: [40/60]	Time 1.414 (5.387)	Loss 0.9349 (1.0094)	Acc@1 84.375 (72.218)	Acc@5 89.062 (92.226)
 * Acc@1 70.288 Acc@5 90.986 Loss 1.10417
Epoch: [5][0/150], lr: 0.00100	Time 68.878 (68.878)	Data 62.049 (62.049)	Loss 0.8183 (0.8183)	Acc@1 78.125 (78.125)	Acc@5 95.312 (95.312)
Epoch: [5][20/150], lr: 0.00100	Time 4.526 (8.288)	Data 0.000 (2.957)	Loss 0.6376 (0.6237)	Acc@1 78.125 (80.432)	Acc@5 98.438 (97.098)
Epoch: [5][40/150], lr: 0.00100	Time 4.824 (6.803)	Data 0.001 (1.516)	Loss 0.3025 (0.5779)	Acc@1 89.062 (81.555)	Acc@5 100.000 (97.713)
Epoch: [5][60/150], lr: 0.00100	Time 6.330 (6.249)	Data 0.001 (1.019)	Loss 0.5544 (0.5789)	Acc@1 85.938 (82.070)	Acc@5 96.875 (97.643)
Epoch: [5][80/150], lr: 0.00100	Time 4.662 (5.991)	Data 0.000 (0.768)	Loss 0.4483 (0.5809)	Acc@1 87.500 (82.292)	Acc@5 96.875 (97.647)
Epoch: [5][100/150], lr: 0.00100	Time 4.956 (5.857)	Data 0.000 (0.617)	Loss 0.5437 (0.5967)	Acc@1 84.375 (81.915)	Acc@5 98.438 (97.447)
Epoch: [5][120/150], lr: 0.00100	Time 4.542 (5.751)	Data 0.000 (0.519)	Loss 0.6057 (0.6109)	Acc@1 79.688 (81.650)	Acc@5 95.312 (97.198)
Epoch: [5][140/150], lr: 0.00100	Time 4.464 (5.664)	Data 0.000 (0.446)	Loss 0.4842 (0.6055)	Acc@1 85.938 (81.848)	Acc@5 98.438 (97.152)
Epoch: [6][0/150], lr: 0.00100	Time 65.025 (65.025)	Data 58.147 (58.147)	Loss 0.6446 (0.6446)	Acc@1 78.125 (78.125)	Acc@5 95.312 (95.312)
Epoch: [6][20/150], lr: 0.00100	Time 8.681 (8.482)	Data 0.010 (2.921)	Loss 0.5930 (0.5156)	Acc@1 84.375 (85.342)	Acc@5 98.438 (97.768)
Epoch: [6][40/150], lr: 0.00100	Time 5.365 (7.027)	Data 0.000 (1.600)	Loss 0.6712 (0.5170)	Acc@1 81.250 (84.985)	Acc@5 96.875 (97.866)
Epoch: [6][60/150], lr: 0.00100	Time 4.601 (6.478)	Data 0.002 (1.077)	Loss 0.3611 (0.4942)	Acc@1 92.188 (85.579)	Acc@5 100.000 (97.925)
Epoch: [6][80/150], lr: 0.00100	Time 4.497 (6.171)	Data 0.000 (0.811)	Loss 0.8036 (0.4969)	Acc@1 75.000 (85.262)	Acc@5 92.188 (97.782)
Epoch: [6][100/150], lr: 0.00100	Time 5.116 (5.983)	Data 0.000 (0.651)	Loss 0.6221 (0.5023)	Acc@1 78.125 (85.071)	Acc@5 95.312 (97.803)
Epoch: [6][120/150], lr: 0.00100	Time 5.746 (5.856)	Data 0.000 (0.544)	Loss 0.7054 (0.5178)	Acc@1 78.125 (84.504)	Acc@5 93.750 (97.650)
Epoch: [6][140/150], lr: 0.00100	Time 4.349 (5.745)	Data 0.000 (0.467)	Loss 0.5603 (0.5228)	Acc@1 82.812 (84.286)	Acc@5 98.438 (97.629)
Epoch: [7][0/150], lr: 0.00100	Time 70.752 (70.752)	Data 63.780 (63.780)	Loss 0.7835 (0.7835)	Acc@1 79.688 (79.688)	Acc@5 98.438 (98.438)
Epoch: [7][20/150], lr: 0.00100	Time 5.926 (8.580)	Data 0.001 (3.041)	Loss 0.5261 (0.4394)	Acc@1 84.375 (88.021)	Acc@5 100.000 (98.512)
Epoch: [7][40/150], lr: 0.00100	Time 4.734 (7.015)	Data 0.001 (1.558)	Loss 0.6796 (0.4642)	Acc@1 78.125 (86.928)	Acc@5 98.438 (98.133)
Epoch: [7][60/150], lr: 0.00100	Time 5.852 (6.444)	Data 0.000 (1.049)	Loss 0.2299 (0.4773)	Acc@1 93.750 (86.194)	Acc@5 100.000 (98.181)
Epoch: [7][80/150], lr: 0.00100	Time 6.272 (6.144)	Data 0.000 (0.790)	Loss 0.2819 (0.4699)	Acc@1 90.625 (86.400)	Acc@5 98.438 (98.129)
Epoch: [7][100/150], lr: 0.00100	Time 5.096 (5.945)	Data 0.000 (0.634)	Loss 0.4677 (0.4662)	Acc@1 87.500 (86.340)	Acc@5 98.438 (98.128)
Epoch: [7][120/150], lr: 0.00100	Time 5.102 (5.806)	Data 0.000 (0.530)	Loss 0.3065 (0.4596)	Acc@1 92.188 (86.493)	Acc@5 98.438 (98.128)
Epoch: [7][140/150], lr: 0.00100	Time 4.470 (5.715)	Data 0.000 (0.456)	Loss 0.5407 (0.4661)	Acc@1 85.938 (86.226)	Acc@5 95.312 (98.005)
Epoch: [8][0/150], lr: 0.00100	Time 71.062 (71.062)	Data 63.777 (63.777)	Loss 0.7005 (0.7005)	Acc@1 84.375 (84.375)	Acc@5 96.875 (96.875)
Epoch: [8][20/150], lr: 0.00100	Time 4.766 (8.268)	Data 0.000 (3.040)	Loss 0.3739 (0.5088)	Acc@1 92.188 (85.268)	Acc@5 95.312 (97.842)
Epoch: [8][40/150], lr: 0.00100	Time 6.336 (6.793)	Data 0.000 (1.558)	Loss 0.3478 (0.4601)	Acc@1 90.625 (86.090)	Acc@5 98.438 (97.980)
Epoch: [8][60/150], lr: 0.00100	Time 7.145 (6.272)	Data 0.001 (1.048)	Loss 0.2972 (0.4502)	Acc@1 89.062 (86.245)	Acc@5 98.438 (97.823)
Epoch: [8][80/150], lr: 0.00100	Time 4.583 (6.012)	Data 0.000 (0.790)	Loss 0.2140 (0.4365)	Acc@1 96.875 (86.671)	Acc@5 100.000 (97.975)
Epoch: [8][100/150], lr: 0.00100	Time 4.861 (5.837)	Data 0.000 (0.634)	Loss 0.4829 (0.4353)	Acc@1 85.938 (86.742)	Acc@5 96.875 (97.958)
Epoch: [8][120/150], lr: 0.00100	Time 6.455 (5.735)	Data 0.000 (0.529)	Loss 0.3866 (0.4232)	Acc@1 89.062 (87.087)	Acc@5 95.312 (98.063)
Epoch: [8][140/150], lr: 0.00100	Time 4.457 (5.636)	Data 0.000 (0.455)	Loss 0.3199 (0.4288)	Acc@1 93.750 (86.968)	Acc@5 100.000 (98.061)
Epoch: [9][0/150], lr: 0.00100	Time 88.594 (88.594)	Data 81.032 (81.032)	Loss 0.2576 (0.2576)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [9][20/150], lr: 0.00100	Time 5.162 (9.259)	Data 0.000 (3.865)	Loss 0.4738 (0.3461)	Acc@1 90.625 (89.881)	Acc@5 95.312 (98.289)
Epoch: [9][40/150], lr: 0.00100	Time 4.682 (7.321)	Data 0.000 (1.981)	Loss 0.3128 (0.3581)	Acc@1 89.062 (88.986)	Acc@5 96.875 (98.514)
Epoch: [9][60/150], lr: 0.00100	Time 4.538 (6.635)	Data 0.000 (1.333)	Loss 0.2159 (0.3692)	Acc@1 96.875 (88.986)	Acc@5 100.000 (98.540)
Epoch: [9][80/150], lr: 0.00100	Time 5.146 (6.275)	Data 0.000 (1.004)	Loss 0.4433 (0.3714)	Acc@1 87.500 (89.062)	Acc@5 96.875 (98.515)
Epoch: [9][100/150], lr: 0.00100	Time 4.668 (6.053)	Data 0.000 (0.806)	Loss 0.3151 (0.3695)	Acc@1 92.188 (89.016)	Acc@5 98.438 (98.546)
Epoch: [9][120/150], lr: 0.00100	Time 4.533 (5.909)	Data 0.000 (0.673)	Loss 0.2807 (0.3767)	Acc@1 92.188 (88.623)	Acc@5 98.438 (98.605)
Epoch: [9][140/150], lr: 0.00100	Time 4.378 (5.803)	Data 0.000 (0.578)	Loss 0.2844 (0.3770)	Acc@1 92.188 (88.475)	Acc@5 100.000 (98.659)
Test: [0/60]	Time 49.522 (49.522)	Loss 0.2158 (0.2158)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Test: [20/60]	Time 1.083 (5.206)	Loss 3.2806 (1.1274)	Acc@1 48.438 (72.098)	Acc@5 64.062 (91.369)
Test: [40/60]	Time 1.073 (4.508)	Loss 1.0514 (1.1041)	Acc@1 82.812 (73.056)	Acc@5 89.062 (92.264)
 * Acc@1 73.434 Acc@5 92.440 Loss 1.08203
Traceback (most recent call last):
  File "main.py", line 390, in <module>
    main()
  File "main.py", line 144, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File "main.py", line 206, in train
    output = model(input)
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 123, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 133, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 77, in parallel_apply
    raise output
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 53, in _worker
    output = module(*input, **kwargs)
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/share/lfs01/workdirs/alex039u2/tsn_paper/server_scripts/real-time-action-recognition/Modified_CNN.py", line 338, in forward
    FProp = self.base_model(input) 
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/share/lfs01/workdirs/alex039u2/tsn_paper/server_scripts/real-time-action-recognition/net/bn_inception.py", line 1300, in forward
    x = self.features(input)
  File "/share/lfs01/workdirs/alex039u2/tsn_paper/server_scripts/real-time-action-recognition/net/bn_inception.py", line 1289, in features
    1
RuntimeError: CUDA error: out of memory
End of Training .................................
