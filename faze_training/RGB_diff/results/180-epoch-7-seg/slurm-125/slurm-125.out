/var/spool/slurmd/job00125/slurm_script: /usr/share/lmod/lmod/libexec/lmod: No such file or directory
Start Training ...................................

          Initializing TSN with base model: BNInception.
          TSN Configurations:
              input_modality:     RGBDiff
              num_segments:       7
              new_length:         5
              consensus_module:   avg
              dropout_ratio:      0.8
               
Load and modify the standard model FC output layer
Dropout Layer added and The modified linear layer is : Linear(in_features=1024, out_features=101, bias=True)
Done. Loading and Modifying 
 ---------------------------------------------------
Converting the ImageNet model to RGBDiff model
The modified 1st layer is Conv2d(15, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))
Done. RGBDiff model is ready.
---------------------------------------------------
group: first_conv_weight has 1 params, lr_mult: 1, decay_mult: 1
group: first_conv_bias has 1 params, lr_mult: 2, decay_mult: 0
group: normal_weight has 69 params, lr_mult: 1, decay_mult: 1
group: normal_bias has 69 params, lr_mult: 2, decay_mult: 0
group: BN scale/shift has 2 params, lr_mult: 1, decay_mult: 0
---------------------------------------------------
Epoch: [0][0/150], lr: 0.00100	Time 109.278 (109.278)	Data 73.215 (73.215)	Loss 4.6284 (4.6284)	Acc@1 0.000 (0.000)	Acc@5 1.562 (1.562)
Epoch: [0][20/150], lr: 0.00100	Time 3.019 (8.998)	Data 0.000 (3.487)	Loss 4.4983 (4.5704)	Acc@1 3.125 (3.199)	Acc@5 14.062 (10.565)
Epoch: [0][40/150], lr: 0.00100	Time 15.930 (7.228)	Data 12.307 (2.529)	Loss 4.1053 (4.4055)	Acc@1 6.250 (5.259)	Acc@5 20.312 (16.387)
Epoch: [0][60/150], lr: 0.00100	Time 14.940 (6.569)	Data 9.973 (2.110)	Loss 3.3035 (4.1585)	Acc@1 18.750 (7.915)	Acc@5 46.875 (23.566)
Epoch: [0][80/150], lr: 0.00100	Time 10.751 (6.106)	Data 7.140 (1.766)	Loss 3.1861 (3.9425)	Acc@1 23.438 (11.285)	Acc@5 53.125 (29.958)
Epoch: [0][100/150], lr: 0.00100	Time 9.273 (5.805)	Data 5.118 (1.537)	Loss 2.8056 (3.7061)	Acc@1 32.812 (15.362)	Acc@5 65.625 (36.572)
Epoch: [0][120/150], lr: 0.00100	Time 8.309 (5.604)	Data 2.321 (1.359)	Loss 2.8402 (3.5193)	Acc@1 31.250 (18.879)	Acc@5 54.688 (41.542)
Epoch: [0][140/150], lr: 0.00100	Time 9.582 (5.433)	Data 6.428 (1.223)	Loss 2.2372 (3.3539)	Acc@1 39.062 (21.797)	Acc@5 75.000 (45.878)
Epoch: [1][0/150], lr: 0.00100	Time 53.927 (53.927)	Data 48.159 (48.159)	Loss 2.1187 (2.1187)	Acc@1 42.188 (42.188)	Acc@5 76.562 (76.562)
Epoch: [1][20/150], lr: 0.00100	Time 3.151 (6.499)	Data 0.003 (2.295)	Loss 1.9083 (1.8759)	Acc@1 50.000 (49.405)	Acc@5 79.688 (80.283)
Epoch: [1][40/150], lr: 0.00100	Time 3.243 (5.331)	Data 0.001 (1.176)	Loss 1.4598 (1.8801)	Acc@1 62.500 (49.162)	Acc@5 89.062 (80.069)
Epoch: [1][60/150], lr: 0.00100	Time 3.372 (4.945)	Data 0.000 (0.791)	Loss 1.6236 (1.8101)	Acc@1 57.812 (51.230)	Acc@5 84.375 (81.506)
Epoch: [1][80/150], lr: 0.00100	Time 9.860 (4.828)	Data 6.075 (0.691)	Loss 1.4007 (1.7475)	Acc@1 57.812 (52.758)	Acc@5 85.938 (82.446)
Epoch: [1][100/150], lr: 0.00100	Time 10.331 (4.803)	Data 6.215 (0.676)	Loss 1.5399 (1.6891)	Acc@1 62.500 (54.223)	Acc@5 84.375 (83.338)
Epoch: [1][120/150], lr: 0.00100	Time 15.350 (4.889)	Data 11.447 (0.777)	Loss 1.6750 (1.6601)	Acc@1 59.375 (54.946)	Acc@5 90.625 (83.807)
Epoch: [1][140/150], lr: 0.00100	Time 33.876 (5.136)	Data 30.743 (1.039)	Loss 1.1521 (1.6121)	Acc@1 65.625 (56.039)	Acc@5 95.312 (84.597)
Epoch: [2][0/150], lr: 0.00100	Time 71.928 (71.928)	Data 65.756 (65.756)	Loss 1.4969 (1.4969)	Acc@1 56.250 (56.250)	Acc@5 87.500 (87.500)
Epoch: [2][20/150], lr: 0.00100	Time 44.596 (10.688)	Data 37.192 (6.471)	Loss 1.2845 (1.2563)	Acc@1 68.750 (63.690)	Acc@5 90.625 (90.179)
Epoch: [2][40/150], lr: 0.00100	Time 16.655 (8.777)	Data 11.031 (4.636)	Loss 1.0604 (1.2289)	Acc@1 67.188 (65.396)	Acc@5 98.438 (90.244)
Epoch: [2][60/150], lr: 0.00100	Time 10.808 (7.448)	Data 5.568 (3.327)	Loss 1.4418 (1.1945)	Acc@1 64.062 (66.112)	Acc@5 85.938 (90.523)
Epoch: [2][80/150], lr: 0.00100	Time 12.872 (6.816)	Data 7.362 (2.692)	Loss 1.0193 (1.1859)	Acc@1 70.312 (66.435)	Acc@5 92.188 (90.721)
Epoch: [2][100/150], lr: 0.00100	Time 12.424 (6.403)	Data 6.521 (2.298)	Loss 1.3131 (1.1514)	Acc@1 60.938 (67.512)	Acc@5 90.625 (91.182)
Epoch: [2][120/150], lr: 0.00100	Time 9.953 (6.111)	Data 4.512 (2.013)	Loss 0.9813 (1.1303)	Acc@1 64.062 (68.066)	Acc@5 96.875 (91.413)
Epoch: [2][140/150], lr: 0.00100	Time 4.449 (5.876)	Data 0.000 (1.782)	Loss 1.2596 (1.1234)	Acc@1 62.500 (68.362)	Acc@5 90.625 (91.467)
Epoch: [3][0/150], lr: 0.00100	Time 44.768 (44.768)	Data 40.295 (40.295)	Loss 0.9272 (0.9272)	Acc@1 78.125 (78.125)	Acc@5 95.312 (95.312)
Epoch: [3][20/150], lr: 0.00100	Time 3.275 (6.432)	Data 0.001 (2.226)	Loss 0.8772 (1.0750)	Acc@1 71.875 (69.494)	Acc@5 93.750 (92.932)
Epoch: [3][40/150], lr: 0.00100	Time 3.063 (5.268)	Data 0.000 (1.141)	Loss 1.0189 (0.9985)	Acc@1 67.188 (71.608)	Acc@5 90.625 (93.331)
Epoch: [3][60/150], lr: 0.00100	Time 3.367 (4.859)	Data 0.000 (0.767)	Loss 0.7944 (0.9782)	Acc@1 75.000 (72.592)	Acc@5 95.312 (93.263)
Epoch: [3][80/150], lr: 0.00100	Time 21.111 (5.062)	Data 18.023 (0.986)	Loss 1.0190 (0.9449)	Acc@1 70.312 (72.936)	Acc@5 87.500 (93.634)
Epoch: [3][100/150], lr: 0.00100	Time 10.004 (5.065)	Data 6.271 (1.016)	Loss 0.7650 (0.9099)	Acc@1 78.125 (74.072)	Acc@5 95.312 (94.028)
Epoch: [3][120/150], lr: 0.00100	Time 9.087 (4.986)	Data 5.607 (0.941)	Loss 0.7236 (0.8975)	Acc@1 76.562 (74.277)	Acc@5 95.312 (94.202)
Epoch: [3][140/150], lr: 0.00100	Time 9.040 (4.920)	Data 5.430 (0.883)	Loss 0.8686 (0.8773)	Acc@1 73.438 (74.900)	Acc@5 96.875 (94.448)
Epoch: [4][0/150], lr: 0.00100	Time 52.334 (52.334)	Data 46.279 (46.279)	Loss 0.7353 (0.7353)	Acc@1 78.125 (78.125)	Acc@5 93.750 (93.750)
Epoch: [4][20/150], lr: 0.00100	Time 4.790 (6.472)	Data 0.014 (2.358)	Loss 0.6355 (0.7852)	Acc@1 82.812 (77.307)	Acc@5 98.438 (96.131)
Epoch: [4][40/150], lr: 0.00100	Time 39.917 (6.367)	Data 33.844 (2.288)	Loss 0.7391 (0.7517)	Acc@1 79.688 (77.515)	Acc@5 96.875 (96.380)
Epoch: [4][60/150], lr: 0.00100	Time 19.625 (6.406)	Data 13.839 (2.339)	Loss 0.8516 (0.7517)	Acc@1 78.125 (78.099)	Acc@5 95.312 (95.927)
Epoch: [4][80/150], lr: 0.00100	Time 9.593 (5.951)	Data 4.190 (1.900)	Loss 0.4606 (0.7275)	Acc@1 89.062 (78.762)	Acc@5 98.438 (96.065)
Epoch: [4][100/150], lr: 0.00100	Time 10.846 (5.720)	Data 5.053 (1.648)	Loss 0.8164 (0.7129)	Acc@1 81.250 (79.285)	Acc@5 92.188 (96.148)
Epoch: [4][120/150], lr: 0.00100	Time 5.585 (5.467)	Data 0.001 (1.377)	Loss 0.8001 (0.7118)	Acc@1 71.875 (79.236)	Acc@5 93.750 (96.152)
Epoch: [4][140/150], lr: 0.00100	Time 4.474 (5.294)	Data 0.000 (1.194)	Loss 0.5020 (0.6971)	Acc@1 84.375 (79.699)	Acc@5 98.438 (96.277)
Test: [0/60]	Time 44.810 (44.810)	Loss 0.6761 (0.6761)	Acc@1 76.562 (76.562)	Acc@5 96.875 (96.875)
Test: [20/60]	Time 24.709 (5.875)	Loss 2.3674 (1.3276)	Acc@1 37.500 (66.592)	Acc@5 70.312 (87.723)
Test: [40/60]	Time 23.475 (4.846)	Loss 1.0260 (1.2213)	Acc@1 79.688 (68.483)	Acc@5 90.625 (88.720)
 * Acc@1 67.618 Acc@5 88.105 Loss 1.26644
Epoch: [5][0/150], lr: 0.00100	Time 42.864 (42.864)	Data 37.475 (37.475)	Loss 0.7173 (0.7173)	Acc@1 81.250 (81.250)	Acc@5 96.875 (96.875)
Epoch: [5][20/150], lr: 0.00100	Time 3.036 (6.287)	Data 0.000 (1.938)	Loss 0.4582 (0.5502)	Acc@1 82.812 (83.333)	Acc@5 98.438 (97.917)
Epoch: [5][40/150], lr: 0.00100	Time 3.269 (5.199)	Data 0.000 (0.994)	Loss 0.6701 (0.5723)	Acc@1 81.250 (83.232)	Acc@5 95.312 (97.409)
Epoch: [5][60/150], lr: 0.00100	Time 3.250 (4.822)	Data 0.000 (0.668)	Loss 0.6161 (0.5955)	Acc@1 79.688 (82.351)	Acc@5 98.438 (97.310)
Epoch: [5][80/150], lr: 0.00100	Time 2.912 (4.637)	Data 0.001 (0.504)	Loss 0.3462 (0.6099)	Acc@1 90.625 (82.137)	Acc@5 100.000 (97.242)
Epoch: [5][100/150], lr: 0.00100	Time 3.057 (4.511)	Data 0.000 (0.404)	Loss 0.5367 (0.6058)	Acc@1 81.250 (82.116)	Acc@5 98.438 (97.401)
Epoch: [5][120/150], lr: 0.00100	Time 3.289 (4.442)	Data 0.000 (0.338)	Loss 0.6375 (0.6106)	Acc@1 81.250 (81.883)	Acc@5 95.312 (97.224)
Epoch: [5][140/150], lr: 0.00100	Time 2.911 (4.380)	Data 0.000 (0.293)	Loss 0.6267 (0.6098)	Acc@1 81.250 (82.070)	Acc@5 95.312 (97.174)
Epoch: [6][0/150], lr: 0.00100	Time 46.224 (46.224)	Data 39.876 (39.876)	Loss 0.5380 (0.5380)	Acc@1 81.250 (81.250)	Acc@5 96.875 (96.875)
Epoch: [6][20/150], lr: 0.00100	Time 9.514 (6.910)	Data 4.526 (2.841)	Loss 0.4208 (0.5013)	Acc@1 87.500 (84.524)	Acc@5 98.438 (98.140)
Epoch: [6][40/150], lr: 0.00100	Time 7.227 (5.580)	Data 1.863 (1.513)	Loss 0.6978 (0.5292)	Acc@1 79.688 (84.184)	Acc@5 93.750 (97.637)
Epoch: [6][60/150], lr: 0.00100	Time 7.820 (5.157)	Data 2.518 (1.092)	Loss 0.3808 (0.5243)	Acc@1 92.188 (84.324)	Acc@5 100.000 (97.669)
Epoch: [6][80/150], lr: 0.00100	Time 10.490 (5.036)	Data 5.040 (0.971)	Loss 0.3879 (0.5180)	Acc@1 90.625 (84.664)	Acc@5 98.438 (97.627)
Epoch: [6][100/150], lr: 0.00100	Time 10.440 (4.887)	Data 5.621 (0.840)	Loss 0.5818 (0.5108)	Acc@1 81.250 (84.947)	Acc@5 96.875 (97.695)
Epoch: [6][120/150], lr: 0.00100	Time 20.109 (5.027)	Data 15.245 (0.992)	Loss 0.4305 (0.5079)	Acc@1 87.500 (84.930)	Acc@5 96.875 (97.740)
Epoch: [6][140/150], lr: 0.00100	Time 14.683 (4.976)	Data 10.041 (0.961)	Loss 0.3547 (0.4979)	Acc@1 89.062 (85.106)	Acc@5 98.438 (97.795)
Epoch: [7][0/150], lr: 0.00100	Time 66.776 (66.776)	Data 63.541 (63.541)	Loss 0.8455 (0.8455)	Acc@1 75.000 (75.000)	Acc@5 96.875 (96.875)
Epoch: [7][20/150], lr: 0.00100	Time 3.728 (6.970)	Data 0.000 (3.028)	Loss 0.4825 (0.5057)	Acc@1 79.688 (84.673)	Acc@5 98.438 (97.693)
Epoch: [7][40/150], lr: 0.00100	Time 8.742 (5.648)	Data 5.092 (1.676)	Loss 0.4194 (0.4691)	Acc@1 89.062 (86.280)	Acc@5 98.438 (97.980)
Epoch: [7][60/150], lr: 0.00100	Time 13.269 (5.351)	Data 9.943 (1.371)	Loss 0.5225 (0.4533)	Acc@1 82.812 (86.603)	Acc@5 96.875 (97.951)
Epoch: [7][80/150], lr: 0.00100	Time 6.852 (5.128)	Data 3.273 (1.133)	Loss 0.5576 (0.4712)	Acc@1 82.812 (85.976)	Acc@5 100.000 (97.955)
Epoch: [7][100/150], lr: 0.00100	Time 5.295 (5.005)	Data 1.811 (1.015)	Loss 0.1976 (0.4610)	Acc@1 95.312 (86.293)	Acc@5 100.000 (98.066)
Epoch: [7][120/150], lr: 0.00100	Time 4.387 (4.891)	Data 0.987 (0.897)	Loss 0.3616 (0.4559)	Acc@1 89.062 (86.364)	Acc@5 100.000 (98.179)
Epoch: [7][140/150], lr: 0.00100	Time 3.967 (4.798)	Data 0.876 (0.811)	Loss 0.4781 (0.4558)	Acc@1 87.500 (86.414)	Acc@5 96.875 (98.238)
Epoch: [8][0/150], lr: 0.00100	Time 52.037 (52.037)	Data 45.782 (45.782)	Loss 0.4572 (0.4572)	Acc@1 87.500 (87.500)	Acc@5 98.438 (98.438)
Epoch: [8][20/150], lr: 0.00100	Time 4.629 (6.296)	Data 0.000 (2.229)	Loss 0.5688 (0.4677)	Acc@1 82.812 (86.830)	Acc@5 96.875 (98.214)
Epoch: [8][40/150], lr: 0.00100	Time 4.877 (5.224)	Data 0.000 (1.187)	Loss 0.5177 (0.4490)	Acc@1 85.938 (87.386)	Acc@5 100.000 (98.399)
Epoch: [8][60/150], lr: 0.00100	Time 5.521 (4.878)	Data 0.000 (0.808)	Loss 0.4926 (0.4527)	Acc@1 82.812 (87.141)	Acc@5 100.000 (98.335)
Epoch: [8][80/150], lr: 0.00100	Time 9.849 (4.739)	Data 4.229 (0.669)	Loss 0.3746 (0.4426)	Acc@1 84.375 (86.979)	Acc@5 100.000 (98.457)
Epoch: [8][100/150], lr: 0.00100	Time 5.226 (4.648)	Data 0.001 (0.575)	Loss 0.3601 (0.4486)	Acc@1 87.500 (86.850)	Acc@5 98.438 (98.407)
Epoch: [8][120/150], lr: 0.00100	Time 6.719 (4.581)	Data 2.283 (0.519)	Loss 0.5585 (0.4498)	Acc@1 87.500 (86.622)	Acc@5 93.750 (98.308)
Epoch: [8][140/150], lr: 0.00100	Time 9.085 (4.548)	Data 4.588 (0.506)	Loss 0.3131 (0.4497)	Acc@1 92.188 (86.580)	Acc@5 98.438 (98.282)
Epoch: [9][0/150], lr: 0.00100	Time 60.784 (60.784)	Data 56.596 (56.596)	Loss 0.4847 (0.4847)	Acc@1 87.500 (87.500)	Acc@5 96.875 (96.875)
Epoch: [9][20/150], lr: 0.00100	Time 3.045 (7.086)	Data 0.000 (3.080)	Loss 0.3216 (0.4216)	Acc@1 89.062 (86.384)	Acc@5 98.438 (98.735)
Epoch: [9][40/150], lr: 0.00100	Time 3.437 (5.621)	Data 0.001 (1.579)	Loss 0.5024 (0.4176)	Acc@1 84.375 (87.348)	Acc@5 98.438 (98.514)
Epoch: [9][60/150], lr: 0.00100	Time 3.029 (5.143)	Data 0.000 (1.062)	Loss 0.3297 (0.3984)	Acc@1 92.188 (88.038)	Acc@5 98.438 (98.668)
Epoch: [9][80/150], lr: 0.00100	Time 17.277 (5.183)	Data 13.959 (1.131)	Loss 0.4990 (0.3883)	Acc@1 82.812 (88.272)	Acc@5 100.000 (98.746)
Epoch: [9][100/150], lr: 0.00100	Time 3.393 (5.033)	Data 0.000 (0.990)	Loss 0.4280 (0.3842)	Acc@1 85.938 (88.475)	Acc@5 96.875 (98.747)
Epoch: [9][120/150], lr: 0.00100	Time 3.473 (4.903)	Data 0.001 (0.861)	Loss 0.5053 (0.3833)	Acc@1 84.375 (88.365)	Acc@5 98.438 (98.735)
Epoch: [9][140/150], lr: 0.00100	Time 4.026 (4.819)	Data 0.536 (0.778)	Loss 0.4769 (0.3803)	Acc@1 81.250 (88.375)	Acc@5 98.438 (98.737)
Test: [0/60]	Time 54.059 (54.059)	Loss 2.1226 (2.1226)	Acc@1 48.438 (48.438)	Acc@5 79.688 (79.688)
Test: [20/60]	Time 22.360 (6.096)	Loss 3.4327 (2.0301)	Acc@1 46.875 (59.152)	Acc@5 64.062 (83.259)
Test: [40/60]	Time 25.282 (5.018)	Loss 1.1339 (1.7162)	Acc@1 78.125 (63.796)	Acc@5 92.188 (86.814)
 * Acc@1 67.275 Acc@5 88.448 Loss 1.53601
Traceback (most recent call last):
  File "main.py", line 390, in <module>
    main()
  File "main.py", line 144, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File "main.py", line 206, in train
    output = model(input)
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 123, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 133, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 77, in parallel_apply
    raise output
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 53, in _worker
    output = module(*input, **kwargs)
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/share/lfs01/workdirs/alex039u2/tsn_paper/server_scripts/real-time-action-recognition/Modified_CNN.py", line 338, in forward
    FProp = self.base_model(input) 
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/share/lfs01/workdirs/alex039u2/tsn_paper/server_scripts/real-time-action-recognition/net/bn_inception.py", line 1300, in forward
    x = self.features(input)
  File "/share/lfs01/workdirs/alex039u2/tsn_paper/server_scripts/real-time-action-recognition/net/bn_inception.py", line 1289, in features
    1
RuntimeError: CUDA error: out of memory
End of Training .................................
