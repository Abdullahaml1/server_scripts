Start Training ...................................

          Initializing TSN with base model: BNInception.
          TSN Configurations:
              input_modality:     RGBDiff
              num_segments:       3
              new_length:         5
              consensus_module:   avg
              dropout_ratio:      0.8
               
Load and modify the standard model FC output layer
Dropout Layer added and The modified linear layer is : Linear(in_features=1024, out_features=101, bias=True)
Done. Loading and Modifying 
 ---------------------------------------------------
Converting the ImageNet model to RGBDiff model
The modified 1st layer is Conv2d(15, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))
Done. RGBDiff model is ready.
---------------------------------------------------
group: first_conv_weight has 1 params, lr_mult: 1, decay_mult: 1
group: first_conv_bias has 1 params, lr_mult: 2, decay_mult: 0
group: normal_weight has 69 params, lr_mult: 1, decay_mult: 1
group: normal_bias has 69 params, lr_mult: 2, decay_mult: 0
group: BN scale/shift has 2 params, lr_mult: 1, decay_mult: 0
---------------------------------------------------
Epoch: [0][0/150], lr: 0.00100	Time 30.721 (30.721)	Data 19.343 (19.343)	Loss 4.6328 (4.6328)	Acc@1 3.125 (3.125)	Acc@5 4.688 (4.688)
Epoch: [0][20/150], lr: 0.00100	Time 18.666 (9.929)	Data 17.254 (8.079)	Loss 4.4723 (4.5776)	Acc@1 4.688 (2.753)	Acc@5 10.938 (9.598)
Epoch: [0][40/150], lr: 0.00100	Time 18.295 (9.674)	Data 16.936 (8.070)	Loss 4.3378 (4.4566)	Acc@1 7.812 (4.802)	Acc@5 20.312 (14.977)
Epoch: [0][60/150], lr: 0.00100	Time 17.437 (9.572)	Data 16.040 (8.032)	Loss 3.7402 (4.2685)	Acc@1 9.375 (6.788)	Acc@5 39.062 (20.517)
Epoch: [0][80/150], lr: 0.00100	Time 17.691 (9.592)	Data 16.396 (8.097)	Loss 3.2902 (4.0607)	Acc@1 18.750 (9.626)	Acc@5 45.312 (26.370)
Epoch: [0][100/150], lr: 0.00100	Time 17.750 (9.563)	Data 16.398 (8.085)	Loss 2.8605 (3.8767)	Acc@1 21.875 (12.175)	Acc@5 56.250 (31.590)
Epoch: [0][120/150], lr: 0.00100	Time 18.547 (9.576)	Data 17.065 (8.117)	Loss 2.8148 (3.7073)	Acc@1 25.000 (15.005)	Acc@5 56.250 (36.015)
Epoch: [0][140/150], lr: 0.00100	Time 17.570 (9.541)	Data 16.245 (8.093)	Loss 2.5326 (3.5591)	Acc@1 37.500 (17.609)	Acc@5 64.062 (39.894)
Epoch: [1][0/150], lr: 0.00100	Time 22.447 (22.447)	Data 20.629 (20.629)	Loss 2.3473 (2.3473)	Acc@1 43.750 (43.750)	Acc@5 70.312 (70.312)
Epoch: [1][20/150], lr: 0.00100	Time 17.329 (9.701)	Data 15.675 (8.312)	Loss 2.2480 (2.4405)	Acc@1 42.188 (36.756)	Acc@5 65.625 (69.420)
Epoch: [1][40/150], lr: 0.00100	Time 16.796 (9.328)	Data 15.222 (7.946)	Loss 1.7935 (2.3690)	Acc@1 46.875 (38.415)	Acc@5 84.375 (70.465)
Epoch: [1][60/150], lr: 0.00100	Time 17.412 (9.272)	Data 15.989 (7.892)	Loss 2.0050 (2.3078)	Acc@1 45.312 (39.677)	Acc@5 76.562 (71.696)
Epoch: [1][80/150], lr: 0.00100	Time 16.052 (9.214)	Data 14.528 (7.837)	Loss 2.2580 (2.2687)	Acc@1 43.750 (40.606)	Acc@5 62.500 (72.473)
Epoch: [1][100/150], lr: 0.00100	Time 16.346 (9.170)	Data 14.935 (7.796)	Loss 1.9742 (2.2362)	Acc@1 46.875 (40.842)	Acc@5 76.562 (73.128)
Epoch: [1][120/150], lr: 0.00100	Time 16.643 (9.145)	Data 15.352 (7.767)	Loss 2.1640 (2.2024)	Acc@1 39.062 (41.451)	Acc@5 70.312 (73.670)
Epoch: [1][140/150], lr: 0.00100	Time 16.155 (9.125)	Data 14.765 (7.747)	Loss 2.2409 (2.1530)	Acc@1 48.438 (42.753)	Acc@5 70.312 (74.601)
Epoch: [2][0/150], lr: 0.00100	Time 21.448 (21.448)	Data 20.076 (20.076)	Loss 1.5887 (1.5887)	Acc@1 59.375 (59.375)	Acc@5 78.125 (78.125)
Epoch: [2][20/150], lr: 0.00100	Time 16.385 (9.319)	Data 15.079 (7.951)	Loss 1.4722 (1.8866)	Acc@1 65.625 (50.446)	Acc@5 81.250 (79.241)
Epoch: [2][40/150], lr: 0.00100	Time 15.666 (9.100)	Data 14.291 (7.723)	Loss 1.8750 (1.8034)	Acc@1 40.625 (51.791)	Acc@5 81.250 (81.364)
Epoch: [2][60/150], lr: 0.00100	Time 16.129 (9.023)	Data 14.838 (7.657)	Loss 1.3125 (1.7572)	Acc@1 64.062 (52.280)	Acc@5 90.625 (82.403)
Epoch: [2][80/150], lr: 0.00100	Time 15.826 (8.964)	Data 14.434 (7.589)	Loss 1.3167 (1.7071)	Acc@1 67.188 (53.569)	Acc@5 89.062 (83.063)
Epoch: [2][100/150], lr: 0.00100	Time 14.994 (8.901)	Data 13.576 (7.533)	Loss 1.6559 (1.6778)	Acc@1 60.938 (54.517)	Acc@5 85.938 (83.369)
Epoch: [2][120/150], lr: 0.00100	Time 15.987 (8.880)	Data 14.555 (7.506)	Loss 1.4081 (1.6747)	Acc@1 64.062 (54.029)	Acc@5 85.938 (83.394)
Epoch: [2][140/150], lr: 0.00100	Time 17.394 (8.866)	Data 16.093 (7.496)	Loss 1.9071 (1.6596)	Acc@1 50.000 (54.566)	Acc@5 76.562 (83.644)
Epoch: [3][0/150], lr: 0.00100	Time 20.045 (20.045)	Data 18.608 (18.608)	Loss 1.3424 (1.3424)	Acc@1 67.188 (67.188)	Acc@5 87.500 (87.500)
Epoch: [3][20/150], lr: 0.00100	Time 16.127 (9.036)	Data 14.603 (7.640)	Loss 1.3610 (1.4763)	Acc@1 57.812 (58.333)	Acc@5 92.188 (87.649)
Epoch: [3][40/150], lr: 0.00100	Time 15.392 (8.789)	Data 13.494 (7.405)	Loss 1.6516 (1.4499)	Acc@1 53.125 (59.223)	Acc@5 87.500 (87.995)
Epoch: [3][60/150], lr: 0.00100	Time 12.087 (8.698)	Data 10.794 (7.330)	Loss 1.3650 (1.4391)	Acc@1 64.062 (59.887)	Acc@5 89.062 (87.910)
Epoch: [3][80/150], lr: 0.00100	Time 11.158 (8.652)	Data 9.864 (7.280)	Loss 1.1666 (1.4287)	Acc@1 65.625 (60.204)	Acc@5 93.750 (87.828)
Epoch: [3][100/150], lr: 0.00100	Time 8.934 (8.599)	Data 7.638 (7.234)	Loss 0.9074 (1.4019)	Acc@1 76.562 (60.922)	Acc@5 95.312 (88.134)
Epoch: [3][120/150], lr: 0.00100	Time 9.248 (8.573)	Data 7.961 (7.207)	Loss 1.1426 (1.3744)	Acc@1 70.312 (61.803)	Acc@5 90.625 (88.262)
Epoch: [3][140/150], lr: 0.00100	Time 8.555 (8.557)	Data 7.204 (7.193)	Loss 1.5204 (1.3687)	Acc@1 59.375 (61.979)	Acc@5 82.812 (88.220)
Epoch: [4][0/150], lr: 0.00100	Time 20.552 (20.552)	Data 19.076 (19.076)	Loss 0.9530 (0.9530)	Acc@1 73.438 (73.438)	Acc@5 93.750 (93.750)
Epoch: [4][20/150], lr: 0.00100	Time 14.897 (8.886)	Data 13.518 (7.512)	Loss 1.6712 (1.2519)	Acc@1 53.125 (64.732)	Acc@5 87.500 (90.402)
Epoch: [4][40/150], lr: 0.00100	Time 14.431 (8.586)	Data 13.075 (7.214)	Loss 1.1650 (1.2322)	Acc@1 67.188 (64.672)	Acc@5 93.750 (90.434)
Epoch: [4][60/150], lr: 0.00100	Time 14.365 (8.517)	Data 13.029 (7.144)	Loss 1.1556 (1.2076)	Acc@1 65.625 (65.446)	Acc@5 96.875 (90.727)
Epoch: [4][80/150], lr: 0.00100	Time 14.366 (8.458)	Data 12.828 (7.086)	Loss 1.1145 (1.2090)	Acc@1 70.312 (65.644)	Acc@5 93.750 (90.741)
Epoch: [4][100/150], lr: 0.00100	Time 15.707 (8.445)	Data 14.347 (7.079)	Loss 1.1398 (1.2022)	Acc@1 73.438 (65.842)	Acc@5 90.625 (90.811)
Epoch: [4][120/150], lr: 0.00100	Time 14.906 (8.419)	Data 13.417 (7.050)	Loss 1.4577 (1.1970)	Acc@1 73.438 (65.883)	Acc@5 85.938 (90.806)
Epoch: [4][140/150], lr: 0.00100	Time 15.750 (8.409)	Data 14.374 (7.040)	Loss 1.1791 (1.1859)	Acc@1 57.812 (66.102)	Acc@5 93.750 (90.891)
Test: [0/60]	Time 23.454 (23.454)	Loss 0.8674 (0.8674)	Acc@1 68.750 (68.750)	Acc@5 98.438 (98.438)
Test: [20/60]	Time 19.465 (10.262)	Loss 2.7813 (1.4258)	Acc@1 29.688 (61.012)	Acc@5 68.750 (87.872)
Test: [40/60]	Time 20.662 (10.190)	Loss 1.2426 (1.3420)	Acc@1 70.312 (62.652)	Acc@5 87.500 (88.034)
 * Acc@1 62.992 Acc@5 88.025 Loss 1.34098
Epoch: [5][0/150], lr: 0.00100	Time 20.055 (20.055)	Data 18.649 (18.649)	Loss 1.0717 (1.0717)	Acc@1 68.750 (68.750)	Acc@5 95.312 (95.312)
Epoch: [5][20/150], lr: 0.00100	Time 14.997 (8.851)	Data 13.483 (7.457)	Loss 1.1086 (1.1039)	Acc@1 60.938 (69.048)	Acc@5 95.312 (91.815)
Epoch: [5][40/150], lr: 0.00100	Time 15.732 (8.648)	Data 14.438 (7.271)	Loss 0.9630 (1.0837)	Acc@1 73.438 (68.902)	Acc@5 93.750 (92.340)
Epoch: [5][60/150], lr: 0.00100	Time 16.417 (8.591)	Data 14.513 (7.217)	Loss 0.9884 (1.0778)	Acc@1 71.875 (69.032)	Acc@5 93.750 (92.520)
Epoch: [5][80/150], lr: 0.00100	Time 14.960 (8.523)	Data 13.665 (7.156)	Loss 1.0055 (1.0680)	Acc@1 76.562 (69.715)	Acc@5 92.188 (92.438)
Epoch: [5][100/150], lr: 0.00100	Time 14.706 (8.489)	Data 13.322 (7.127)	Loss 1.1230 (1.0572)	Acc@1 67.188 (69.957)	Acc@5 90.625 (92.450)
Epoch: [5][120/150], lr: 0.00100	Time 15.078 (8.467)	Data 13.700 (7.105)	Loss 1.0120 (1.0472)	Acc@1 70.312 (70.377)	Acc@5 93.750 (92.510)
Epoch: [5][140/150], lr: 0.00100	Time 14.915 (8.452)	Data 13.518 (7.092)	Loss 0.9758 (1.0523)	Acc@1 68.750 (70.279)	Acc@5 93.750 (92.442)
Epoch: [6][0/150], lr: 0.00100	Time 19.939 (19.939)	Data 18.567 (18.567)	Loss 1.1133 (1.1133)	Acc@1 71.875 (71.875)	Acc@5 92.188 (92.188)
Epoch: [6][20/150], lr: 0.00100	Time 15.181 (8.785)	Data 13.713 (7.427)	Loss 1.2581 (1.0070)	Acc@1 65.625 (72.247)	Acc@5 95.312 (92.634)
Epoch: [6][40/150], lr: 0.00100	Time 15.394 (8.477)	Data 14.042 (7.122)	Loss 0.7915 (0.9877)	Acc@1 75.000 (71.951)	Acc@5 96.875 (93.064)
Epoch: [6][60/150], lr: 0.00100	Time 14.875 (8.360)	Data 13.479 (7.001)	Loss 1.0586 (0.9706)	Acc@1 73.438 (72.285)	Acc@5 92.188 (93.007)
Epoch: [6][80/150], lr: 0.00100	Time 15.672 (8.332)	Data 14.225 (6.969)	Loss 0.8433 (0.9669)	Acc@1 71.875 (72.319)	Acc@5 95.312 (93.113)
Epoch: [6][100/150], lr: 0.00100	Time 15.160 (8.339)	Data 13.746 (6.981)	Loss 0.6865 (0.9669)	Acc@1 76.562 (72.308)	Acc@5 98.438 (93.209)
Epoch: [6][120/150], lr: 0.00100	Time 14.793 (8.317)	Data 13.413 (6.961)	Loss 1.0783 (0.9596)	Acc@1 70.312 (72.366)	Acc@5 93.750 (93.246)
Epoch: [6][140/150], lr: 0.00100	Time 14.647 (8.306)	Data 13.342 (6.953)	Loss 0.9854 (0.9450)	Acc@1 70.312 (72.806)	Acc@5 90.625 (93.418)
Epoch: [7][0/150], lr: 0.00100	Time 20.477 (20.477)	Data 18.949 (18.949)	Loss 0.8967 (0.8967)	Acc@1 76.562 (76.562)	Acc@5 90.625 (90.625)
Epoch: [7][20/150], lr: 0.00100	Time 15.538 (8.619)	Data 13.999 (7.243)	Loss 0.6790 (0.9167)	Acc@1 84.375 (73.512)	Acc@5 93.750 (94.717)
Epoch: [7][40/150], lr: 0.00100	Time 14.396 (8.342)	Data 13.102 (6.981)	Loss 1.0205 (0.8438)	Acc@1 70.312 (75.534)	Acc@5 95.312 (94.931)
Epoch: [7][60/150], lr: 0.00100	Time 15.102 (8.311)	Data 13.722 (6.944)	Loss 0.7473 (0.8456)	Acc@1 79.688 (75.487)	Acc@5 96.875 (94.595)
Epoch: [7][80/150], lr: 0.00100	Time 14.872 (8.287)	Data 13.388 (6.919)	Loss 1.1888 (0.8525)	Acc@1 70.312 (75.154)	Acc@5 93.750 (94.676)
Epoch: [7][100/150], lr: 0.00100	Time 14.595 (8.264)	Data 13.232 (6.899)	Loss 0.5884 (0.8330)	Acc@1 84.375 (75.650)	Acc@5 96.875 (94.879)
Epoch: [7][120/150], lr: 0.00100	Time 15.134 (8.252)	Data 13.764 (6.888)	Loss 0.4782 (0.8260)	Acc@1 87.500 (75.736)	Acc@5 96.875 (94.951)
Epoch: [7][140/150], lr: 0.00100	Time 15.027 (8.239)	Data 13.731 (6.875)	Loss 0.6984 (0.8213)	Acc@1 79.688 (75.931)	Acc@5 100.000 (95.024)
Epoch: [8][0/150], lr: 0.00100	Time 20.157 (20.157)	Data 18.452 (18.452)	Loss 0.7004 (0.7004)	Acc@1 82.812 (82.812)	Acc@5 96.875 (96.875)
Epoch: [8][20/150], lr: 0.00100	Time 14.624 (8.538)	Data 13.209 (7.165)	Loss 0.6533 (0.7732)	Acc@1 78.125 (77.604)	Acc@5 98.438 (95.908)
Traceback (most recent call last):
  File "main.py", line 391, in <module>
    main()
  File "main.py", line 144, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File "main.py", line 194, in train
    for i, (input, target) in enumerate(train_loader):
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 637, in __next__
    return self._process_next_batch(batch)
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 658, in _process_next_batch
    raise batch.exc_type(batch.exc_msg)
FileNotFoundError: Traceback (most recent call last):
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 138, in _worker_loop
    samples = collate_fn([dataset[i] for i in batch_indices])
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 138, in <listcomp>
    samples = collate_fn([dataset[i] for i in batch_indices])
  File "/lfs01/workdirs/alex039/alex039u2/tsn_paper/server_scripts/real-time-action-recognition/UCF_Dataset.py", line 137, in __getitem__
    return self.Vid2Frames(vid_info, indices)
  File "/lfs01/workdirs/alex039/alex039u2/tsn_paper/server_scripts/real-time-action-recognition/UCF_Dataset.py", line 116, in Vid2Frames
    seg_imgs = [Image.open(os.path.join(info.path, self.image_prefix.format(p))).convert('RGB')]
  File "/home/alex039u2/data/anaconda3/lib/python3.7/site-packages/PIL/Image.py", line 2580, in open
    fp = builtins.open(filename, "rb")
FileNotFoundError: [Errno 2] No such file or directory: '/home/alex039u2/data/tsn_paper/datasets/jpegs_256/v_LongJump_g18_c03/frame000110.jpg'

End of Training .................................
